# Checklist

Checklists: https://secartifacts.github.io/usenixsec2023/tips

Unfortunately, artifacts sometimes miss badges because they were not tested on a clean setup, or not documented enough, or because running experiments is too error-prone due to complex manual steps. This year, we provide checklists for authors and evaluators to help prepare and evaluate artifacts, minimizing the risk of an artifact unnecessarily missing a badge.

## Artifact Available

    [X]The artifact is available on a public website with a specific version such as a git commit
    [X]The artifact has a “read me” file with a reference to the paper
    [X]Ideally, the artifact should have a license that at least allows use for comparison purposes

    [X]Artifacts must meet these criteria at the time of evaluation. Promises of future availability, such as artifacts “temporarily” gated behind credentials given to evaluators, are not enough.

## Artifact Functional

    [X]The artifact has a “read me” file with high-level documentation:
        [X]A description, such as which folders correspond to code, benchmarks, data, …
        [X]A list of supported environments, including OS, specific hardware if necessary, …
        [X]Compilation and running instructions, including dependencies and pre-installation steps, with a reasonable degree of automation such as scripts to download and build exotic dependencies
        [X]Configuration instructions, such as selecting IP addresses or disks
        [X]Usage instructions, such as analyzing a new data set
        [X]Instructions for a “minimal working example”
    [X]The artifact has documentation explaining the high-level organization of modules, and code comments explaining non-obvious code, such that other researchers can fully understand it
    [X]The artifact contains all components the paper describes using the same terminology as the paper, and no obsolete code/data
    [N/A]If the artifact includes a container/VM, it must also contain a script to create it from scratch

    [X]Artifacts must be usable on other machines than the authors’, though they may require hardware such as specific network cards. Information such as IP addresses must not be hardcoded.

## Results Reproduced

    [X]The artifact has a “read me” file that documents:
        [X]The exact environment the authors used, including OS version and any special hardware
        [X]The exact commands to run to reproduce each claim from the paper
        [X]The approximate resources used per claim, such as “5 minutes, 1 GB of disk space”
        [X]The scripts to reproduce claims are documented, allowing researchers to ensure they correspond to the claims; merely producing the right output is not enough
    [X]The artifact’s external dependencies are fetched from well-known sources such as official websites or GitHub repositories
        [N/A]Changes to such dependencies should be clearly separated, such as using a patch file or a repository fork with a clear commit history

    [X]The amount of manual work, such as writing configuration files, should be reasonably minimized. In particular, there should be no redundant manual steps such as writing the same configuration values in multiple places, as this inevitably leads to human error.

# Things to include (public)

[X]sink server
[X]affected-norand
[X]affected-payload
[X]test-entropy
[X]test-printable
[X]test-prefixes
[X]detect.py

# Things ONLY for artifact eval (not public)

[X]ssh private keys for the VMs (1 DigitalOcean VM and 1 Alibaba VM) + info how to ssh

# After being granted the badgets

[] Add the badgets to the self-hosted usenix security paper: https://secartifacts.github.io/usenixsec2023/instructions
[] Append the ARTIFACT APPENDIX to the paper appendix.
